{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-analyse_Ngoahitsi_Yvan_et_Mohamed_Yehdih_Mohamed_Youssouf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJM1JQLxcULr"
      },
      "source": [
        "Etape 1: Import et paramètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bQVkH3fal0k"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.datasets.imdb as imdb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os,sys\n",
        "from importlib import reload\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"CPU is\", \"available\" if tf.config.list_physical_devices('CPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M1yHVD7dyq9"
      },
      "source": [
        "1.1 Fixation des paramètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDFxwqOed0ZJ"
      },
      "source": [
        "vocab_size           = 10000\n",
        "hide_most_frequently = 0\n",
        "\n",
        "epochs     = 10\n",
        "batch_size = 512\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igmd-gjdev3B"
      },
      "source": [
        "1.2Comprendre le 'One-hot encoding'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nscXF0FlhJWj"
      },
      "source": [
        "sentence = \"I've never seen a movie like this before\"\n",
        "\n",
        "dictionary  = {\"a\":0, \"before\":1, \"fantastic\":2, \"i've\":3, \"is\":4, \"like\":5, \"movie\":6, \"never\":7, \"seen\":8, \"this\":9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIxvfGR9hVTn"
      },
      "source": [
        "sentence_words = sentence.lower().split()\n",
        "\n",
        "sentence_vect  = [ dictionary[w] for w in sentence_words ]\n",
        "\n",
        "print('Words sentence are         : ', sentence_words)\n",
        "print('Our vectorized sentence is : ', sentence_vect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsOb5K9WhmhN"
      },
      "source": [
        "Ensuite, nous codons notre phrase vectorisée comme un tenseur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N--WaPM9hpGx"
      },
      "source": [
        "# ---- We get a (sentence length x vector size) matrix of zeros\n",
        "#\n",
        "onehot = np.zeros( (10,8) )\n",
        "\n",
        "# ---- We set some 1 for each word\n",
        "#\n",
        "for i,w in enumerate(sentence_vect):\n",
        "    onehot[w,i]=1\n",
        "\n",
        "# --- Show it\n",
        "#\n",
        "print('In a basic way :\\n\\n', onehot, '\\n\\nWith a pandas wiew :\\n')\n",
        "data={ f'{sentence_words[i]:.^10}':onehot[:,i] for i,w in enumerate(sentence_vect) }\n",
        "df=pd.DataFrame(data)\n",
        "df.index=dictionary.keys()\n",
        "df.style.set_precision(0).highlight_max(axis=0).set_properties(**{'text-align': 'center'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5dnFismixaP"
      },
      "source": [
        "Etape 2.\n",
        "1. Récuperation de données\n",
        "\n",
        "L'ensemble de données est composé de 2 parties :\n",
        "\n",
        "•\tPour les critiques, ce sera notre x ;\n",
        "\n",
        "•\tPour les avis (positifs/négatifs), ce sera notre y.\n",
        "\n",
        "Il y a aussi un dictionnaire, car les mots sont indexés dans les commentaires.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKu7aL8PrKwy"
      },
      "source": [
        "\n",
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzOOy8atrkgf"
      },
      "source": [
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))\n",
        "print(\"\\n\")\n",
        "train_examples[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn_p10-Ai7ya"
      },
      "source": [
        "Etape 2\n",
        "2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd1eUCpi3OT"
      },
      "source": [
        "# ----- Retrieve x,y\n",
        "#\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data( num_words=vocab_size, skip_top=hide_most_frequently)\n",
        "\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_test  = np.asarray(y_test ).astype('float32')\n",
        "\n",
        "# ---- About\n",
        "\n",
        "print(\"x_train : {}  y_train : {}\".format(x_train.shape, y_train.shape))\n",
        "print(\"x_test  : {}  y_test  : {}\".format(x_test.shape,  y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9vhemzevcU9"
      },
      "source": [
        "Etape 3: A propos de notre Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLpUt9szkt0-"
      },
      "source": [
        "3.1.Codages des phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ERKzpXUkyDE"
      },
      "source": [
        "print('\\nReview example (x_train[12]) :\\n\\n',x_train[12])\n",
        "print('\\nOpinions (y_train) :\\n\\n',y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO9n2Lt1lDpU"
      },
      "source": [
        "3.2.Chargement du dictionnaire "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjztnFklMsg"
      },
      "source": [
        "# ---- Retrieve dictionary {word:index}, and encode it in ascii\n",
        "#\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# ---- Shift the dictionary from +3\n",
        "#\n",
        "word_index = {w:(i+3) for w,i in word_index.items()}\n",
        "\n",
        "# ---- Add <pad>, <start> and <unknown> tags\n",
        "#\n",
        "word_index.update( {'<pad>':0, '<start>':1, '<unknown>':2, '<undef>':3,} )\n",
        "\n",
        "# ---- Create a reverse dictionary : {index:word}\n",
        "#\n",
        "index_word = {index:word for word,index in word_index.items()} \n",
        "\n",
        "# ---- About dictionary\n",
        "#\n",
        "print('\\nDictionary size     : ', len(word_index))\n",
        "print('\\nSmall extract :\\n')\n",
        "for k in range(440,460):print(f'    {k:2d} : {index_word[k]}' )\n",
        "\n",
        "# ---- Add a nice function to transpose :\n",
        "#\n",
        "def dataset2text(review):\n",
        "    return ' '.join([index_word.get(i, '?') for i in review])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqJyRTAJle_6"
      },
      "source": [
        "3.3 Jetez un coup d'œil, pour les humains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc_fcXHH2Lkq"
      },
      "source": [
        "print('Review example :')\n",
        "print(x_train[12])\n",
        "print(\"\\n\")\n",
        "print('After translation:')\n",
        "print(dataset2text(x_train[12]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o_DEg-BnRD9"
      },
      "source": [
        "3.4. Quelques statistiques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvqlVOa2nVG6"
      },
      "source": [
        "sizes=[len(i) for i in x_train]\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.hist(sizes, bins=400)\n",
        "plt.gca().set(title='Distribution of reviews by size - [{:5.2f}, {:5.2f}]'.format(min(sizes),max(sizes)), \n",
        "              xlabel='Size', ylabel='Density', xlim=[0,1500])\n",
        "#pwk.save_fig('01-stats-sizes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8TeBGsPn5Ry"
      },
      "source": [
        "unk=[ 100*(s.count(2)/len(s)) for s in x_train]\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.hist(unk, bins=100)\n",
        "plt.gca().set(title='Percent of unknown words - [{:5.2f}, {:5.2f}]'.format(min(unk),max(unk)), \n",
        "              xlabel='# unknown', ylabel='Density', xlim=[0,30])\n",
        "#pwk.save_fig('02-stats-unknown')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGQKq2xy7NNd"
      },
      "source": [
        "Etape 4:\n",
        "Approche de base.\n",
        "\n",
        "Chaque phrase est codée avec un vecteur de longueur égale à la taille du dictionnaire.\n",
        "\n",
        "Ainsi, chaque phrase sera codée avec un vecteur simple.\n",
        "La valeur de chaque composant est 0 si le mot n'est pas présent dans la phrase ou 1 si le mot est présent.\n",
        "\n",
        "Pour une phrase s=[3,4,7] et un dictionnaire de 10 mots...\n",
        "Nous aurons un vecteur v=[0,0,0,1,1,0,0,1,0,0,0,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcRsoy9yoIbR"
      },
      "source": [
        "4.1 Our one hot encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87MXeHmqoMHA"
      },
      "source": [
        "def one_hot_encoder(x, vector_size=10000):\n",
        "    \n",
        "    # ---- Set all to 0\n",
        "    #\n",
        "    x_encoded = np.zeros((len(x), vector_size))\n",
        "    \n",
        "    # ---- For each sentence\n",
        "    #\n",
        "    for i,sentence in enumerate(x):\n",
        "        for word in sentence:\n",
        "            x_encoded[i, word] = 1.\n",
        "\n",
        "    return x_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4IQ7KYpohsC"
      },
      "source": [
        "4.2. Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt1M3aMWoktO"
      },
      "source": [
        "x_train = one_hot_encoder(x_train)\n",
        "x_test  = one_hot_encoder(x_test)\n",
        "\n",
        "print(\"To have a look, x_train[12] became :\", x_train[12] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReCkwyR8ovYr"
      },
      "source": [
        "Etape 5:Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBgaQ4TWoxuo"
      },
      "source": [
        "def get_model(vector_size=10000):\n",
        "    \n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Input( shape=(vector_size,) ))\n",
        "    model.add(keras.layers.Dense( 32, activation='relu'))\n",
        "    model.add(keras.layers.Dense( 32, activation='relu'))\n",
        "    model.add(keras.layers.Dense( 1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(optimizer = 'rmsprop',\n",
        "                  loss      = 'binary_crossentropy',\n",
        "                  metrics   = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkQMZvv0Jeo8"
      },
      "source": [
        "6.Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfllcScnrRSZ"
      },
      "source": [
        "6.1 Get it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-hJ8YyrUfl"
      },
      "source": [
        "model = get_model(vector_size=vocab_size)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bfNGL-2rgIr"
      },
      "source": [
        "6.2 Add Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcrdA1q9roNg"
      },
      "source": [
        "os.makedirs(f'/models',   mode=0o750, exist_ok=True)\n",
        "save_dir = f'/models/best_model.h5'\n",
        "savemodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I-h_Y-RslLd"
      },
      "source": [
        "6.3 Train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h22C91JxsrnW"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs          = epochs,\n",
        "                    batch_size      = batch_size,\n",
        "                    validation_data = (x_test, y_test),\n",
        "                    verbose         = 1,\n",
        "                    callbacks       = [savemodel_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUdhsQY-CiQT"
      },
      "source": [
        "Etape 7: Evaluation du résultat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwBaaFSPuHyV"
      },
      "source": [
        "7.1Historique de l'entrainement et création d'un graphique d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdlC5dh_uAuY"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kotYfVIhufIB"
      },
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOngdDrix7hR"
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl4IoGvTyUXL"
      },
      "source": [
        "Dans ce graphique, les points représentent la perte et la précision de la formation, et les lignes pleines représentent la perte de validation et la précision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtWdlEzNymG2"
      },
      "source": [
        "Etape 8: Reload and evaluate best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvJ_dRh0ym_9"
      },
      "source": [
        "model = keras.models.load_model(f'/models/best_model.h5')\n",
        "\n",
        "# ---- Evaluate\n",
        "score  = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
        "print('x_test / accuracy  : {:5.4f}'.format(score[1]))\n",
        "\n",
        "values=[score[1], 1-score[1]]\n",
        "\n",
        "\n",
        "# ---- Confusion matrix\n",
        "\n",
        "y_sigmoid = model.predict(x_test)\n",
        "\n",
        "y_pred = y_sigmoid.copy()\n",
        "y_pred[ y_sigmoid< 0.5 ] = 0\n",
        "y_pred[ y_sigmoid>=0.5 ] = 1    \n",
        "\n",
        "confusion_matrix(y_test,y_pred,labels=range(2))\n",
        "confusion_matrix(y_test,y_pred,range(2), normalize='true' )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}